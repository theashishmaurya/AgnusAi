# ─── AgnusAI Docker Compose Configuration ──────────────────────────────────────
# Traefik gateway automatically exposes:
#   - Main API:     $PUBLIC_URL (default: http://localhost:3000)
#   - Dashboard:    /dashboard
#   - Docs:         /docs
#   - Traefik UI:   /api/default/agnus@docker (when TRAEFIK_UI=true)
#
# To use with Azure (or any cloud LLM), remove the ollama service and set:
#   LLM_PROVIDER=azure
#   EMBEDDING_PROVIDER=azure
# (No Ollama needed if using cloud for both LLM and embeddings)
# ------------------------------------------------------------------------------

services:
  traefik:
    image: traefik:v3.6.9
    command:
      - "--api.dashboard=true"
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--accesslog=true"
      - "--log.level=${TRAEFIK_LOG_LEVEL:-INFO}"
    ports:
      - "80:80"
      - "443:443"
      - "8080:8080" # Traefik dashboard (insecure, for debugging)
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
    labels:
      - "traefik.enable=false"
    restart: unless-stopped
    networks:
      - agnus-network

  agnus:
    build: .
    environment:
      - DATABASE_URL=postgres://agnus:agnus@postgres:5432/agnus
      - ADMIN_EMAIL=${ADMIN_EMAIL:-admin@example.com}
      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-changeme}
      - JWT_SECRET=${JWT_SECRET:-change-me-in-production}
      - WEBHOOK_SECRET=${WEBHOOK_SECRET}
      - SESSION_SECRET=${SESSION_SECRET}
      - API_KEY=${API_KEY:-}
      - LLM_PROVIDER=${LLM_PROVIDER:-ollama}
      - LLM_MODEL=${LLM_MODEL:-qwen3.5:397b-cloud}
      # Ollama
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL:-http://host.docker.internal:11434/v1}
      # OpenAI
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      # Azure OpenAI
      - AZURE_OPENAI_ENDPOINT=${AZURE_OPENAI_ENDPOINT:-}
      - AZURE_OPENAI_API_KEY=${AZURE_OPENAI_API_KEY:-}
      - AZURE_API_VERSION=${AZURE_API_VERSION:-2025-01-01-preview}
      # Anthropic / Claude
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      # Custom OpenAI-compatible endpoint
      - CUSTOM_LLM_URL=${CUSTOM_LLM_URL:-}
      - CUSTOM_LLM_API_KEY=${CUSTOM_LLM_API_KEY:-}
      - EMBEDDING_PROVIDER=${EMBEDDING_PROVIDER:-}
      - EMBEDDING_BASE_URL=${EMBEDDING_BASE_URL:-}
      - EMBEDDING_MODEL=${EMBEDDING_MODEL:-}
      - EMBEDDING_API_KEY=${EMBEDDING_API_KEY:-}
      - REVIEW_DEPTH=${REVIEW_DEPTH:-standard}
      - GITHUB_TOKEN=${GITHUB_TOKEN:-}
      # PUBLIC_URL - user-facing URL for feedback links
      - PUBLIC_URL=${PUBLIC_URL:-http://localhost:3000}
      # BASE_URL - internal URL for Docker routing (defaults to PUBLIC_URL if not set)
      - BASE_URL=${BASE_URL:-${PUBLIC_URL:-http://localhost:3000}}
      - FEEDBACK_SECRET=${FEEDBACK_SECRET:-}
      # Docker internal DNS resolution
      - HOST_ID=host.docker.internal
    volumes:
      - /tmp:/tmp:ro
      - repos-data:/repos
    depends_on:
      postgres:
        condition: service_healthy
    restart: unless-stopped
    labels:
      - "traefik.enable=true"
      # Main API route
      - "traefik.http.routers.agnus-api.rule=PathPrefix(`/`)"
      - "traefik.http.routers.agnus-api.entrypoints=web"
      - "traefik.http.routers.agnus-api.service=agnus-api"
      - "traefik.http.services.agnus-api.loadbalancer.server.port=3000"
      # Health check endpoint
      - "traefik.http.routers.agnus-health.rule=Path(`/health`)"
      - "traefik.http.routers.agnus-health.entrypoints=web"
      - "traefik.http.routers.agnus-health.service=agnus-health"
      - "traefik.http.services.agnus-health.loadbalancer.server.port=3000"
    networks:
      - agnus-network

  postgres:
    image: pgvector/pgvector:pg16
    environment:
      - POSTGRES_USER=agnus
      - POSTGRES_PASSWORD=agnus
      - POSTGRES_DB=agnus
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U agnus"]
      interval: 5s
      timeout: 5s
      retries: 10
    restart: unless-stopped
    networks:
      - agnus-network

  # Ollama - LOCAL LLM SERVER (optional)
  # Remove this section if using cloud LLM (Azure/OpenAI/Claude)
  # If using Azure embeddings, you can also remove this entirely
  # ollama:
  #   image: ollama/ollama:latest
  #   volumes:
  #     - ollama-data:/root/.ollama
  #   ports:
  #     - "11434:11434"
  #   restart: unless-stopped
  #   networks:
  #     - agnus-network

networks:
  agnus-network:
    driver: bridge

volumes:
  postgres-data:
  repos-data:
  # ollama-data:  # Uncomment if using Ollama